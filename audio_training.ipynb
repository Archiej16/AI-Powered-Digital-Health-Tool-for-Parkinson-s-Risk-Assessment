{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8ae49b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required libraries are already installed.\n",
      "All libraries are ready.\n",
      "All libraries imported successfully.\n",
      "Dataset 1 'parkinsons_multimodal_data' already exists.\n",
      "Dataset 2 directory 'parkinsons_audio_data_2' already exists.\n",
      "--- Data setup complete ---\n",
      "\n",
      "Loading Dataset 1...\n",
      "Added 73 files from Dataset 1.\n",
      "\n",
      "Loading Dataset 2 (Scanning for files)...\n",
      "  Found 41 files in 'hc_ah' -> Labeled as 0\n",
      "  Found 40 files in 'pd_ah' -> Labeled as 1\n",
      "Added 81 files from Dataset 2.\n",
      "\n",
      "✅ Total Valid Audio Files: 154\n",
      "\n",
      "Starting Feature Extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [06:37<00:00,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features for 73 files.\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "✅ New Model Accuracy: 80.00%\n",
      "\n",
      "✅ SUCCESS! Models saved to current folder:\n",
      "   - audio_model.joblib\n",
      "   - audio_scaler.joblib\n",
      "\n",
      "NEXT STEP: Run 'setup_models.py' to move these to your backend.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Install Required Libraries ---\n",
    "import subprocess\n",
    "import sys\n",
    "import os \n",
    "import zipfile \n",
    "\n",
    "try:\n",
    "    # Check for all required libraries\n",
    "    import numpy\n",
    "    import pandas\n",
    "    import matplotlib\n",
    "    import seaborn\n",
    "    import sklearn\n",
    "    import tqdm\n",
    "    import librosa\n",
    "    import xgboost\n",
    "    import joblib\n",
    "    print(\"All required libraries are already installed.\")\n",
    "except ImportError:\n",
    "    print(\"Installing required libraries...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"librosa\", \"xgboost\", \"scikit-learn\", \"tqdm\", \"seaborn\", \"pandas\", \"numpy\", \"matplotlib\", \"joblib\"])\n",
    "\n",
    "print(\"All libraries are ready.\")\n",
    "\n",
    "# --- 2. Import All Libraries ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from tqdm import tqdm \n",
    "import librosa\n",
    "import xgboost \n",
    "import joblib \n",
    "\n",
    "print(\"All libraries imported successfully.\")\n",
    "\n",
    "\n",
    "# --- 3. Unzip Datasets (Recursive) ---\n",
    "\n",
    "def find_file(filename, search_paths):\n",
    "    for path in search_paths:\n",
    "        full_path = os.path.join(path, filename)\n",
    "        if os.path.exists(full_path):\n",
    "            return full_path\n",
    "    return None\n",
    "\n",
    "search_locations = ['.', '..', '../backend', 'backend']\n",
    "\n",
    "# 3.1 Dataset 1 (Original)\n",
    "zip_name_1 = 'combined_dastaset_1.zip'\n",
    "zip_path_1 = find_file(zip_name_1, search_locations)\n",
    "data_dir_1 = 'parkinsons_multimodal_data'\n",
    "\n",
    "if zip_path_1 and not os.path.exists(data_dir_1):\n",
    "    print(f\"Unzipping '{zip_name_1}'...\")\n",
    "    with zipfile.ZipFile(zip_path_1, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_dir_1)\n",
    "elif os.path.exists(data_dir_1):\n",
    "    print(f\"Dataset 1 '{data_dir_1}' already exists.\")\n",
    "\n",
    "# 3.2 Dataset 2 (audio_2.zip)\n",
    "zip_name_2 = 'audio_2.zip'\n",
    "zip_path_2 = find_file(zip_name_2, search_locations)\n",
    "data_dir_2 = 'parkinsons_audio_data_2'\n",
    "\n",
    "if zip_path_2:\n",
    "    if not os.path.exists(data_dir_2):\n",
    "        print(f\"Unzipping '{zip_name_2}'...\")\n",
    "        try:\n",
    "            with zipfile.ZipFile(zip_path_2, 'r') as zip_ref:\n",
    "                zip_ref.extractall(data_dir_2)\n",
    "            print(f\"Dataset 2 unzipped successfully.\")\n",
    "            \n",
    "            # --- RECURSIVE UNZIP START ---\n",
    "            print(\"Scanning for nested zip files inside Dataset 2...\")\n",
    "            for root, dirs, files in os.walk(data_dir_2):\n",
    "                for file in files:\n",
    "                    if file.lower().endswith('.zip'):\n",
    "                        nested_zip_path = os.path.join(root, file)\n",
    "                        print(f\"  Found nested zip: {file}. Unzipping...\")\n",
    "                        try:\n",
    "                            with zipfile.ZipFile(nested_zip_path, 'r') as z:\n",
    "                                z.extractall(root)\n",
    "                            print(f\"  ✅ Unzipped {file}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"  ❌ Failed to unzip {file}: {e}\")\n",
    "            # --- RECURSIVE UNZIP END ---\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error unzipping {zip_name_2}: {e}\")\n",
    "    else:\n",
    "        print(f\"Dataset 2 directory '{data_dir_2}' already exists.\")\n",
    "else:\n",
    "    print(f\"WARNING: '{zip_name_2}' not found in {search_locations}. Please ensure it is in your project folder.\")\n",
    "\n",
    "print(\"--- Data setup complete ---\")\n",
    "\n",
    "\n",
    "# --- 4. Load Audio File Paths and Labels ---\n",
    "all_audio_paths = []\n",
    "all_labels = []\n",
    "\n",
    "# Load Dataset 1 (Known Structure)\n",
    "print(\"\\nLoading Dataset 1...\")\n",
    "ds1_base = os.path.join(data_dir_1, 'Parkinson Multi Model DATASET')\n",
    "ds1_map = {\n",
    "    \"Healthy/AUDIO 1 HEALTHY\": 0, \"Healthy/AUDIO 2 HEALTHY\": 0,\n",
    "    \"Unhealthy/AUDIO 1 UNHEALTHY\": 1, \"Unhealthy/AUDIO 2 UNHEALTHY\": 1\n",
    "}\n",
    "initial_count = len(all_audio_paths)\n",
    "for folder, label in ds1_map.items():\n",
    "    path = os.path.join(ds1_base, folder)\n",
    "    if os.path.exists(path):\n",
    "        for f in os.listdir(path):\n",
    "            if f.lower().endswith('.wav'):\n",
    "                all_audio_paths.append(os.path.join(path, f))\n",
    "                all_labels.append(label)\n",
    "print(f\"Added {len(all_audio_paths) - initial_count} files from Dataset 1.\")\n",
    "\n",
    "# Load Dataset 2 (Smart Scan)\n",
    "print(\"\\nLoading Dataset 2 (Scanning for files)...\")\n",
    "initial_count_ds2 = len(all_audio_paths)\n",
    "\n",
    "if os.path.exists(data_dir_2):\n",
    "    for root, dirs, files in os.walk(data_dir_2):\n",
    "        wav_files = [f for f in files if f.lower().endswith('.wav')]\n",
    "        if not wav_files: continue\n",
    "            \n",
    "        folder_name = os.path.basename(root).lower()\n",
    "        label = None\n",
    "        \n",
    "        # Smart Labeling Keywords\n",
    "        if any(x in folder_name for x in ['non', 'healthy', 'control', 'hc', 'normal']):\n",
    "            label = 0\n",
    "        elif any(x in folder_name for x in ['pd', 'parkinson', 'unhealthy', 'positive']):\n",
    "            label = 1\n",
    "            \n",
    "        if label is not None:\n",
    "            print(f\"  Found {len(wav_files)} files in '{folder_name}' -> Labeled as {label}\")\n",
    "            for f in wav_files:\n",
    "                all_audio_paths.append(os.path.join(root, f))\n",
    "                all_labels.append(label)\n",
    "else:\n",
    "    print(\"Dataset 2 folder not found.\")\n",
    "\n",
    "count_ds2 = len(all_audio_paths) - initial_count_ds2\n",
    "print(f\"Added {count_ds2} files from Dataset 2.\")\n",
    "\n",
    "if len(all_audio_paths) == 0:\n",
    "    print(\"\\n❌ ERROR: No audio files found! Check your paths/zip files.\")\n",
    "else:\n",
    "    print(f\"\\n✅ Total Valid Audio Files: {len(all_audio_paths)}\")\n",
    "\n",
    "\n",
    "# --- 5. Feature Extraction ---\n",
    "def extract_features(file_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, duration=30, sr=None)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)\n",
    "        mel = np.mean(librosa.feature.melspectrogram(y=y, sr=sr).T, axis=0)\n",
    "        contrast = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr).T, axis=0)\n",
    "        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr).T, axis=0)\n",
    "        spec_cent = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr).T, axis=0)\n",
    "        spec_bw = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr).T, axis=0)\n",
    "        rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr).T, axis=0)\n",
    "        zcr = np.mean(librosa.feature.zero_crossing_rate(y).T, axis=0)\n",
    "        return np.concatenate((mfccs, chroma, mel, contrast, tonnetz, spec_cent, spec_bw, rolloff, zcr))\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "print(\"\\nStarting Feature Extraction...\")\n",
    "feature_list = []\n",
    "label_list = []\n",
    "\n",
    "for path, label in tqdm(zip(all_audio_paths, all_labels), total=len(all_audio_paths)):\n",
    "    feats = extract_features(path)\n",
    "    if feats is not None:\n",
    "        feature_list.append(feats)\n",
    "        label_list.append(label)\n",
    "\n",
    "print(f\"Extracted features for {len(feature_list)} files.\")\n",
    "\n",
    "\n",
    "# --- 6. Train & Save ---\n",
    "if len(feature_list) > 0:\n",
    "    X = np.array(feature_list)\n",
    "    y = np.array(label_list)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(\"\\nTraining Random Forest...\")\n",
    "    model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    acc = accuracy_score(y_test, model.predict(X_test_scaled))\n",
    "    print(f\"\\n✅ New Model Accuracy: {acc*100:.2f}%\")\n",
    "    \n",
    "    # SAVE TO CURRENT DIRECTORY with NEW NAME\n",
    "    joblib.dump(model, 'audio_model.joblib')\n",
    "    joblib.dump(scaler, 'audio_scaler.joblib')\n",
    "    \n",
    "    print(\"\\n✅ SUCCESS! Models saved to current folder:\")\n",
    "    print(\"   - audio_model.joblib\")\n",
    "    print(\"   - audio_scaler.joblib\")\n",
    "    print(\"\\nNEXT STEP: Run 'setup_models.py' to move these to your backend.\")\n",
    "else:\n",
    "    print(\"❌ Extraction failed. No models saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
